# Research Paper Summaries

## ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain

**Authors**: Haochen Zhao, Xiangru Tang, Ziran Yang, Xiao Han, Xuanzhi Feng, Yueqing Fan, Senhao Cheng, Di Jin, Yilun Zhao, Arman Cohan, Mark Gerstein

**Publication Date**: 2024-11-23T12:50:33Z

### Summary
Researchers have introduced ChemSafetyBench to evaluate the accuracy and safety of large language models in chemistry, finding that existing LLMs often generate incorrect or unsafe responses, which can encourage dangerous behavior.

---

## A Flexible Large Language Models Guardrail Development Methodology
  Applied to Off-Topic Prompt Detection

**Authors**: Gabriel Chua, Shing Yee Chan, Shaun Khoo

**Publication Date**: 2024-11-20T00:31:23Z

### Summary
Large Language Models are prone to misusing tasks beyond their intended scope, with current guardrails suffering from high false-positive rates and requiring real-world data. A new methodology addresses these challenges by generating diverse prompts through an LLM, constructing a synthetic dataset, and training off-topic guardrails that outperform heuristic approaches and generalize to harmful prompts.

---

## Diversity Helps Jailbreak Large Language Models

**Authors**: Weiliang Zhao, Daniel Ben-Levi, Junfeng Yang, Chengzhi Mao

**Publication Date**: 2024-11-06T19:39:48Z

### Summary
A powerful jailbreak technique has been discovered that leverages large language models' ability to diverge from prior context, bypassing safety constraints and generating harmful outputs, which outperforms existing approaches by achieving a 62% higher success rate in compromising leading chatbots.

---

## Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM
  Safety Alignment

**Authors**: Jason Vega, Junsheng Huang, Gaokai Zhang, Hangoo Kang, Minjia Zhang, Gagandeep Singh

**Publication Date**: 2024-11-05T03:51:13Z

### Summary
Simple random augmentations to input prompts can bypass safety alignment in state-of-the-art Large Language Models, allowing low-resource and unsophisticated attackers (referred to as "stochastic monkeys") to significantly improve their chances of bypassing alignment with only 25 random augmentations per prompt.

---

## Chat Bankman-Fried: an Exploration of LLM Alignment in Finance

**Authors**: Claudia Biancotti, Carolina Camassa, Andrea Coletta, Oliver Giudice, Aldo Glielmo

**Publication Date**: 2024-11-01T08:56:17Z

### Summary
The study proposes an experimental framework to assess large language models' (LLMs) adherence to ethical standards in finance by simulating scenarios where they are asked to prioritize debt repayment over customer assets. The results show significant heterogeneity in the LLMs' propensity for unethical behavior, with factors such as risk aversion and regulatory environment influencing their decisions.

---

