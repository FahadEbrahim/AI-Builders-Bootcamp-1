{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4ef775-778d-4c0f-b901-65df2ba5caf5",
   "metadata": {},
   "source": [
    "# Summarizing Research Papers with GPT-4o\n",
    "## ABB #1 - Session 3\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf11ab2-418e-4ea3-a3b8-2d2a09232c88",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b91857-1b75-4b5e-ac03-0bb05bf9e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "from top_secret import my_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db523c7-0b98-402d-91c4-d5e4c21c2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup api client\n",
    "client = OpenAI(api_key=my_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d88996-64c0-43b0-af2a-0a4af4f46152",
   "metadata": {},
   "source": [
    "### 1) Extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06f8b84-23cf-49c3-a024-ebf98b1b8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"papers/attention-is-all-you-need.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42881f0-32bf-4733-809a-b290118c9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = fitz.open(filepath)\n",
    "text = \"\".join([page.get_text() for page in pdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb264250-d62c-472b-ad3e-21bbd263551e",
   "metadata": {},
   "source": [
    "### 2) Write prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aaab4a9-cfe5-4488-87b3-c173fc96320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"# System Role: Research Paper Summarizer\n",
    "\n",
    "Your task is to act as an academic summarizer, condensing research papers into concise, clear, and accessible summaries. Follow these guidelines:\n",
    "\n",
    "## Structure\n",
    "\n",
    "Always use the following structure for your summary:\n",
    "\n",
    "- **Title**: The paper's title.\n",
    "- **Authors**: Key authors and their affiliations (if available).\n",
    "- **Abstract**: A paraphrased version of the paper's abstract.\n",
    "- **Objective**: The research goal or question being addressed.\n",
    "- **Methodology**: Key methods or experiments used.\n",
    "- **Findings**: Main results or insights.\n",
    "- **Conclusion**: The authors' conclusion and implications.\n",
    "- **Relevance**: Why this research is significant or what problem it addresses.\n",
    "\n",
    "## Style\n",
    "\n",
    "- Write in a neutral and academic tone.\n",
    "- Use simple, precise language to ensure clarity for a broad audience.\n",
    "\n",
    "## Length\n",
    "\n",
    "- Keep summaries concise (150-300 words) unless otherwise specified.\n",
    "\n",
    "## Audience\n",
    "\n",
    "- Assume the audience has general technical knowledge but may not be familiar with the specific field of the paper.\n",
    "\n",
    "## Special Instructions\n",
    "\n",
    "- If specific sections of the paper are missing, skip them and note it clearly (e.g., \"Authors section not specified\").\n",
    "- Avoid subjective opinions or interpretations beyond the paper's content.\n",
    "\n",
    "## Citations\n",
    "\n",
    "- Provide clear citations in APA format (if needed).\n",
    "\n",
    "---\n",
    "\n",
    "You are summarizing the following research paper:\n",
    "\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc17b34-940f-4ff3-ad67-3b2fd7ee5a48",
   "metadata": {},
   "source": [
    "### 3) Summarize Paper with GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a27209d-5711-4eab-9828-97d301d17bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Title**: Attention Is All You Need\n",
      "- **Authors**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin (Google Brain, Google Research, University of Toronto)\n",
      "- **Abstract**: This paper introduces the Transformer, a new architecture for sequence transduction that relies entirely on attention mechanisms, eliminating the need for recurrence and convolutions. The Transformer demonstrates superior performance in machine translation tasks, achieving state-of-the-art BLEU scores for English-to-German and English-to-French translations, while also being more efficient in training time and parallelization.\n",
      "\n",
      "- **Objective**: The primary goal of this research is to propose a novel model architecture for sequence transduction that improves upon existing recurrent and convolutional models by leveraging attention mechanisms exclusively.\n",
      "\n",
      "- **Methodology**: The Transformer model is structured with an encoder-decoder architecture, where both components utilize multi-head self-attention and feed-forward neural networks. The authors conducted experiments on the WMT 2014 English-to-German and English-to-French translation tasks, comparing the performance and training efficiency of the Transformer against existing state-of-the-art models.\n",
      "\n",
      "- **Findings**: The Transformer achieved a BLEU score of 28.4 for English-to-German and 41.8 for English-to-French translations, surpassing previous models by significant margins and requiring less training time. The model also showed strong generalization to other tasks, such as English constituency parsing.\n",
      "\n",
      "- **Conclusion**: The authors conclude that the Transformer model, by relying solely on attention mechanisms, not only achieves state-of-the-art performance in translation tasks but also offers advantages in training speed and efficiency. They express optimism about the potential applications of attention-based models in various domains beyond text.\n",
      "\n",
      "- **Relevance**: This research is significant as it introduces a transformative approach to sequence modeling, addressing limitations of traditional recurrent and convolutional networks, and lays the groundwork for further advancements in natural language processing and other fields requiring sequence transduction.\n",
      "\n",
      "**Citation**: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2023). Attention Is All You Need. 31st Conference on Neural Information Processing Systems (NIPS 2017). arXiv:1706.03762v7 [cs.CL].\n"
     ]
    }
   ],
   "source": [
    "# make api call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Research paper summarizer.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], \n",
    "    temperature = 0.5\n",
    ")\n",
    "\n",
    "# extract response\n",
    "summary = response.choices[0].message.content\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e89e07-f2d5-4eec-8f83-ba8da7fb2772",
   "metadata": {},
   "source": [
    "### 4) Display Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a058e3ea-480e-4f61-96cf-ddfced60c94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Title**: Attention Is All You Need\n",
       "- **Authors**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin (Google Brain, Google Research, University of Toronto)\n",
       "- **Abstract**: This paper introduces the Transformer, a new architecture for sequence transduction that relies entirely on attention mechanisms, eliminating the need for recurrence and convolutions. The Transformer demonstrates superior performance in machine translation tasks, achieving state-of-the-art BLEU scores for English-to-German and English-to-French translations, while also being more efficient in training time and parallelization.\n",
       "\n",
       "- **Objective**: The primary goal of this research is to propose a novel model architecture for sequence transduction that improves upon existing recurrent and convolutional models by leveraging attention mechanisms exclusively.\n",
       "\n",
       "- **Methodology**: The Transformer model is structured with an encoder-decoder architecture, where both components utilize multi-head self-attention and feed-forward neural networks. The authors conducted experiments on the WMT 2014 English-to-German and English-to-French translation tasks, comparing the performance and training efficiency of the Transformer against existing state-of-the-art models.\n",
       "\n",
       "- **Findings**: The Transformer achieved a BLEU score of 28.4 for English-to-German and 41.8 for English-to-French translations, surpassing previous models by significant margins and requiring less training time. The model also showed strong generalization to other tasks, such as English constituency parsing.\n",
       "\n",
       "- **Conclusion**: The authors conclude that the Transformer model, by relying solely on attention mechanisms, not only achieves state-of-the-art performance in translation tasks but also offers advantages in training speed and efficiency. They express optimism about the potential applications of attention-based models in various domains beyond text.\n",
       "\n",
       "- **Relevance**: This research is significant as it introduces a transformative approach to sequence modeling, addressing limitations of traditional recurrent and convolutional networks, and lays the groundwork for further advancements in natural language processing and other fields requiring sequence transduction.\n",
       "\n",
       "**Citation**: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2023). Attention Is All You Need. 31st Conference on Neural Information Processing Systems (NIPS 2017). arXiv:1706.03762v7 [cs.CL]."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aad5dc-bbc8-4ac9-8421-1e9ae83fcdc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
