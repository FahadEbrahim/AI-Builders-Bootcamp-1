{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install requests"
      ],
      "metadata": {
        "id": "pHPBGxLYnuTR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET"
      ],
      "metadata": {
        "id": "eeDSbg-On6mm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NJc5Vbr5iUay"
      },
      "outputs": [],
      "source": [
        "def search_arxiv_to_print(topic):\n",
        "    url = \"https://export.arxiv.org/api/query\"\n",
        "    params = {\n",
        "        \"search_query\": f'all:%22{topic}%22',\n",
        "        \"start\": 0,\n",
        "        \"max_results\": 10,\n",
        "        \"sortBy\": \"submittedDate\",\n",
        "        \"sortOrder\": \"descending\",\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        root = ET.fromstring(response.content)\n",
        "        entries = root.findall('{http://www.w3.org/2005/Atom}entry')\n",
        "\n",
        "        for entry in entries:\n",
        "            title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
        "            authors = ', '.join([author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')])\n",
        "            abstract = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
        "\n",
        "            print(f\"Title:\\n{title}\\n\")\n",
        "            print(f\"Authors:\\n{authors}\\n\")\n",
        "            print(f\"Abstract:\\n{abstract}\\n\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_arxiv_to_print(\"LLMs safety\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RabPN_C3mbAI",
        "outputId": "3a213248-337d-415e-eced-3246fc2c32d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title:\n",
            "Diversity Helps Jailbreak Large Language Models\n",
            "\n",
            "Authors:\n",
            "Weiliang Zhao, Daniel Ben-Levi, Junfeng Yang, Chengzhi Mao\n",
            "\n",
            "Abstract:\n",
            "  We have uncovered a powerful jailbreak technique that leverages large\n",
            "language models' ability to diverge from prior context, enabling them to bypass\n",
            "safety constraints and generate harmful outputs. By simply instructing the LLM\n",
            "to deviate and obfuscate previous attacks, our method dramatically outperforms\n",
            "existing approaches, achieving up to a 62% higher success rate in compromising\n",
            "nine leading chatbots, including GPT-4, Gemini, and Llama, while using only 13%\n",
            "of the queries. This revelation exposes a critical flaw in current LLM safety\n",
            "training, suggesting that existing methods may merely mask vulnerabilities\n",
            "rather than eliminate them. Our findings sound an urgent alarm for the need to\n",
            "revolutionize testing methodologies to ensure robust and reliable LLM security.\n",
            "\n",
            "\n",
            "==================================================\n",
            "Title:\n",
            "Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM\n",
            "  Safety Alignment\n",
            "\n",
            "Authors:\n",
            "Jason Vega, Junsheng Huang, Gaokai Zhang, Hangoo Kang, Minjia Zhang, Gagandeep Singh\n",
            "\n",
            "Abstract:\n",
            "  Safety alignment of Large Language Models (LLMs) has recently become a\n",
            "critical objective of model developers. In response, a growing body of work has\n",
            "been investigating how safety alignment can be bypassed through various\n",
            "jailbreaking methods, such as adversarial attacks. However, these jailbreak\n",
            "methods can be rather costly or involve a non-trivial amount of creativity and\n",
            "effort, introducing the assumption that malicious users are high-resource or\n",
            "sophisticated. In this paper, we study how simple random augmentations to the\n",
            "input prompt affect safety alignment effectiveness in state-of-the-art LLMs,\n",
            "such as Llama 3 and Qwen 2. We perform an in-depth evaluation of 17 different\n",
            "models and investigate the intersection of safety under random augmentations\n",
            "with multiple dimensions: augmentation type, model size, quantization,\n",
            "fine-tuning-based defenses, and decoding strategies (e.g., sampling\n",
            "temperature). We show that low-resource and unsophisticated attackers, i.e.\n",
            "$\\textit{stochastic monkeys}$, can significantly improve their chances of\n",
            "bypassing alignment with just 25 random augmentations per prompt.\n",
            "\n",
            "\n",
            "==================================================\n",
            "Title:\n",
            "SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and\n",
            "  Prompt Types\n",
            "\n",
            "Authors:\n",
            "Yutao Mou, Shikun Zhang, Wei Ye\n",
            "\n",
            "Abstract:\n",
            "  Ensuring the safety of large language model (LLM) applications is essential\n",
            "for developing trustworthy artificial intelligence. Current LLM safety\n",
            "benchmarks have two limitations. First, they focus solely on either\n",
            "discriminative or generative evaluation paradigms while ignoring their\n",
            "interconnection. Second, they rely on standardized inputs, overlooking the\n",
            "effects of widespread prompting techniques, such as system prompts, few-shot\n",
            "demonstrations, and chain-of-thought prompting. To overcome these issues, we\n",
            "developed SG-Bench, a novel benchmark to assess the generalization of LLM\n",
            "safety across various tasks and prompt types. This benchmark integrates both\n",
            "generative and discriminative evaluation tasks and includes extended data to\n",
            "examine the impact of prompt engineering and jailbreak on LLM safety. Our\n",
            "assessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the\n",
            "benchmark reveals that most LLMs perform worse on discriminative tasks than\n",
            "generative ones, and are highly susceptible to prompts, indicating poor\n",
            "generalization in safety alignment. We also explain these findings\n",
            "quantitatively and qualitatively to provide insights for future research.\n",
            "\n",
            "\n",
            "==================================================\n",
            "Title:\n",
            "Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n",
            "  Mirroring\n",
            "\n",
            "Authors:\n",
            "Honglin Mu, Han He, Yuxin Zhou, Yunlong Feng, Yang Xu, Libo Qin, Xiaoming Shi, Zeming Liu, Xudong Han, Qi Shi, Qingfu Zhu, Wanxiang Che\n",
            "\n",
            "Abstract:\n",
            "  Large language model (LLM) safety is a critical issue, with numerous studies\n",
            "employing red team testing to enhance model security. Among these, jailbreak\n",
            "methods explore potential vulnerabilities by crafting malicious prompts that\n",
            "induce model outputs contrary to safety alignments. Existing black-box\n",
            "jailbreak methods often rely on model feedback, repeatedly submitting queries\n",
            "with detectable malicious instructions during the attack search process.\n",
            "Although these approaches are effective, the attacks may be intercepted by\n",
            "content moderators during the search process. We propose an improved transfer\n",
            "attack method that guides malicious prompt construction by locally training a\n",
            "mirror model of the target black-box model through benign data distillation.\n",
            "This method offers enhanced stealth, as it does not involve submitting\n",
            "identifiable malicious instructions to the target model during the search\n",
            "phase. Our approach achieved a maximum attack success rate of 92%, or a\n",
            "balanced value of 80% with an average of 1.5 detectable jailbreak queries per\n",
            "sample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\n",
            "the need for more robust defense mechanisms.\n",
            "\n",
            "\n",
            "==================================================\n",
            "Title:\n",
            "Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities\n",
            "\n",
            "Authors:\n",
            "Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao\n",
            "\n",
            "Abstract:\n",
            "  Recent research has shown that Large Language Models (LLMs) are vulnerable to\n",
            "automated jailbreak attacks, where adversarial suffixes crafted by algorithms\n",
            "appended to harmful queries bypass safety alignment and trigger unintended\n",
            "responses. Current methods for generating these suffixes are computationally\n",
            "expensive and have low Attack Success Rates (ASR), especially against\n",
            "well-aligned models like Llama2 and Llama3. To overcome these limitations, we\n",
            "introduce ADV-LLM, an iterative self-tuning process that crafts adversarial\n",
            "LLMs with enhanced jailbreak ability. Our framework significantly reduces the\n",
            "computational cost of generating adversarial suffixes while achieving nearly\n",
            "100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack\n",
            "transferability to closed-source models, achieving 99% ASR on GPT-3.5 and 49%\n",
            "ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving\n",
            "jailbreak ability, ADV-LLM provides valuable insights for future safety\n",
            "alignment research through its ability to generate large datasets for studying\n",
            "LLM safety.\n",
            "\n",
            "\n",
            "==================================================\n",
            "Title:\n",
            "SafetyAnalyst: Interpretable, transparent, and steerable LLM safety\n",
            "  moderation\n",
            "\n",
            "Authors:\n",
            "Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine\n",
            "\n",
            "Abstract:\n",
            "  The ideal LLM content moderation system would be both structurally\n",
            "interpretable (so its decisions can be explained to users) and steerable (to\n",
            "reflect a community's values or align to safety standards). However, current\n",
            "systems fall short on both of these dimensions. To address this gap, we present\n",
            "SafetyAnalyst, a novel LLM safety moderation framework. Given a prompt,\n",
            "SafetyAnalyst creates a structured \"harm-benefit tree,\" which identifies 1) the\n",
            "actions that could be taken if a compliant response were provided, 2) the\n",
            "harmful and beneficial effects of those actions (along with their likelihood,\n",
            "severity, and immediacy), and 3) the stakeholders that would be impacted by\n",
            "those effects. It then aggregates this structured representation into a\n",
            "harmfulness score based on a parameterized set of safety preferences, which can\n",
            "be transparently aligned to particular values. Using extensive harm-benefit\n",
            "features generated by SOTA LLMs on 19k prompts, we fine-tuned an open-weight LM\n",
            "to specialize in generating harm-benefit trees through symbolic knowledge\n",
            "distillation. On a comprehensive set of prompt safety benchmarks, we show that\n",
            "our system (average F1=0.75) outperforms existing LLM safety moderation systems\n",
            "(average F1$<$0.72) on prompt harmfulness classification, while offering the\n",
            "additional advantages of interpretability and steerability.\n",
            "\n",
            "\n",
            "==================================================\n",
            "Title:\n",
            "Bayesian scaling laws for in-context learning\n",
            "\n",
            "Authors:\n",
            "Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah D. Goodman\n",
            "\n",
            "Abstract:\n",
            "  In-context learning (ICL) is a powerful technique for getting language models\n",
            "to perform complex tasks with no training updates. Prior work has established\n",
            "strong correlations between the number of in-context examples provided and the\n",
            "accuracy of the model's predictions. In this paper, we seek to explain this\n",
            "correlation by showing that ICL approximates a Bayesian learner. This\n",
            "perspective gives rise to a family of novel Bayesian scaling laws for ICL. In\n",
            "experiments with \\mbox{GPT-2} models of different sizes, our scaling laws\n",
            "exceed or match existing scaling laws in accuracy while also offering\n",
            "interpretable terms for task priors, learning efficiency, and per-example\n",
            "probabilities. To illustrate the analytic power that such interpretable scaling\n",
            "laws provide, we report on controlled synthetic dataset experiments designed to\n",
            "inform real-world studies of safety alignment. In our experimental protocol, we\n",
            "use SFT to suppress an unwanted existing model capability and then use ICL to\n",
            "try to bring that capability back (many-shot jailbreaking). We then experiment\n",
            "on real-world instruction-tuned LLMs using capabilities benchmarks as well as a\n",
            "new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws\n",
            "accurately predict the conditions under which ICL will cause the suppressed\n",
            "behavior to reemerge, which sheds light on the ineffectiveness of post-training\n",
            "at increasing LLM safety.\n",
            "\n",
            "\n",
            "==================================================\n",
            "Title:\n",
            "NetSafe: Exploring the Topological Safety of Multi-agent Networks\n",
            "\n",
            "Authors:\n",
            "Miao Yu, Shilong Wang, Guibin Zhang, Junyuan Mao, Chenlong Yin, Qijiong Liu, Qingsong Wen, Kun Wang, Yang Wang\n",
            "\n",
            "Abstract:\n",
            "  Large language models (LLMs) have empowered nodes within multi-agent networks\n",
            "with intelligence, showing growing applications in both academia and industry.\n",
            "However, how to prevent these networks from generating malicious information\n",
            "remains unexplored with previous research on single LLM's safety be challenging\n",
            "to transfer. In this paper, we focus on the safety of multi-agent networks from\n",
            "a topological perspective, investigating which topological properties\n",
            "contribute to safer networks. To this end, we propose a general framework,\n",
            "NetSafe along with an iterative RelCom interaction to unify existing diverse\n",
            "LLM-based agent frameworks, laying the foundation for generalized topological\n",
            "safety research. We identify several critical phenomena when multi-agent\n",
            "networks are exposed to attacks involving misinformation, bias, and harmful\n",
            "information, termed as Agent Hallucination and Aggregation Safety. Furthermore,\n",
            "we find that highly connected networks are more susceptible to the spread of\n",
            "adversarial attacks, with task performance in a Star Graph Topology decreasing\n",
            "by 29.7%. Besides, our proposed static metrics aligned more closely with\n",
            "real-world dynamic evaluations than traditional graph-theoretic metrics,\n",
            "indicating that networks with greater average distances from attackers exhibit\n",
            "enhanced safety. In conclusion, our work introduces a new topological\n",
            "perspective on the safety of LLM-based multi-agent networks and discovers\n",
            "several unreported phenomena, paving the way for future research to explore the\n",
            "safety of such networks.\n",
            "\n",
            "\n",
            "==================================================\n",
            "Title:\n",
            "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models\n",
            "\n",
            "Authors:\n",
            "Benji Peng, Ziqian Bi, Qian Niu, Ming Liu, Pohsun Feng, Tianyang Wang, Lawrence K. Q. Yan, Yizhu Wen, Yichao Zhang, Caitlyn Heqi Yin\n",
            "\n",
            "Abstract:\n",
            "  Large Language Models (LLMs) have transformed artificial intelligence by\n",
            "advancing natural language understanding and generation, enabling applications\n",
            "across fields beyond healthcare, software engineering, and conversational\n",
            "systems. Despite these advancements in the past few years, LLMs have shown\n",
            "considerable vulnerabilities, particularly to prompt injection and jailbreaking\n",
            "attacks. This review analyzes the state of research on these vulnerabilities\n",
            "and presents available defense strategies. We roughly categorize attack\n",
            "approaches into prompt-based, model-based, multimodal, and multilingual,\n",
            "covering techniques such as adversarial prompting, backdoor injections, and\n",
            "cross-modality exploits. We also review various defense mechanisms, including\n",
            "prompt filtering, transformation, alignment techniques, multi-agent defenses,\n",
            "and self-regulation, evaluating their strengths and shortcomings. We also\n",
            "discuss key metrics and benchmarks used to assess LLM safety and robustness,\n",
            "noting challenges like the quantification of attack success in interactive\n",
            "contexts and biases in existing datasets. Identifying current research gaps, we\n",
            "suggest future directions for resilient alignment strategies, advanced defenses\n",
            "against evolving attacks, automation of jailbreak detection, and consideration\n",
            "of ethical and societal impacts. This review emphasizes the need for continued\n",
            "research and cooperation within the AI community to enhance LLM security and\n",
            "ensure their safe deployment.\n",
            "\n",
            "\n",
            "==================================================\n",
            "Title:\n",
            "Jailbreaking LLM-Controlled Robots\n",
            "\n",
            "Authors:\n",
            "Alexander Robey, Zachary Ravichandran, Vijay Kumar, Hamed Hassani, George J. Pappas\n",
            "\n",
            "Abstract:\n",
            "  The recent introduction of large language models (LLMs) has revolutionized\n",
            "the field of robotics by enabling contextual reasoning and intuitive\n",
            "human-robot interaction in domains as varied as manipulation, locomotion, and\n",
            "self-driving vehicles. When viewed as a stand-alone technology, LLMs are known\n",
            "to be vulnerable to jailbreaking attacks, wherein malicious prompters elicit\n",
            "harmful text by bypassing LLM safety guardrails. To assess the risks of\n",
            "deploying LLMs in robotics, in this paper, we introduce RoboPAIR, the first\n",
            "algorithm designed to jailbreak LLM-controlled robots. Unlike existing, textual\n",
            "attacks on LLM chatbots, RoboPAIR elicits harmful physical actions from\n",
            "LLM-controlled robots, a phenomenon we experimentally demonstrate in three\n",
            "scenarios: (i) a white-box setting, wherein the attacker has full access to the\n",
            "NVIDIA Dolphins self-driving LLM, (ii) a gray-box setting, wherein the attacker\n",
            "has partial access to a Clearpath Robotics Jackal UGV robot equipped with a\n",
            "GPT-4o planner, and (iii) a black-box setting, wherein the attacker has only\n",
            "query access to the GPT-3.5-integrated Unitree Robotics Go2 robot dog. In each\n",
            "scenario and across three new datasets of harmful robotic actions, we\n",
            "demonstrate that RoboPAIR, as well as several static baselines, finds\n",
            "jailbreaks quickly and effectively, often achieving 100% attack success rates.\n",
            "Our results reveal, for the first time, that the risks of jailbroken LLMs\n",
            "extend far beyond text generation, given the distinct possibility that\n",
            "jailbroken robots could cause physical damage in the real world. Indeed, our\n",
            "results on the Unitree Go2 represent the first successful jailbreak of a\n",
            "deployed commercial robotic system. Addressing this emerging vulnerability is\n",
            "critical for ensuring the safe deployment of LLMs in robotics. Additional media\n",
            "is available at: https://robopair.org\n",
            "\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}